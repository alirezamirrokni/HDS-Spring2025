# High-Dimensional Statistics (HDS) — Sharif University of Technology

**Instructor:** Amir Najafi  
**Head TA:** Alireza Mirrokni

This repository hosts materials for the High-Dimensional Statistics (HDS) course at Sharif University of Technology. It includes reference papers discussed in class, theoretical problem sets, and slide decks. Materials are provided for educational use only.

---

## Repository layout

```
.
├── Assignment Papers/   # Reference and background papers discussed in class
├── Assignments/         # Theoretical problem sets (no coding assignments here)
└── Presentations/       # Lecture slides and student presentations
```

> Note: The **Assignments** folder contains **theory-only** exercises. If coding material is added later, it will live in a separate folder and be linked here.

---

## Syllabus (English overview)

- **Foundations & Motivation**  
  High-dimensional vs. classical statistics, the curse of dimensionality, non‑asymptotic viewpoints, and why structural assumptions matter.

- **Concentration Inequalities**  
  Sub-Gaussian and sub-exponential tails, Chernoff/Hoeffding/Bernstein bounds, Lipschitz functions of Gaussians, and martingale concentration (bounded differences, Azuma–Hoeffding).

- **Concentration of Measure & Information-Theoretic Tools**  
  Entropy methods, Talagrand-type lemmas, transportation (Wasserstein) distances, and information inequalities.

- **Uniform Laws & Complexity**  
  Glivenko–Cantelli convergence, the Dvoretzky–Kiefer–Wolfowitz (DKW) inequality, Rademacher complexity, and Wasserstein metrics in generalization.

- **Metric Entropy & Chaining**  
  Covering/packing numbers, Dudley’s entropy integral, generic chaining for sub-Gaussian processes, and Orlicz norms.

- **Random Matrices & Covariance Estimation**  
  Basic ensembles and eigenvalue behavior, sub-Gaussian models, high-dimensional covariance estimation with performance bounds, and structured estimators (low rank / sparsity).

- **Sparse Linear Models in High Dimensions**  
  Exact/noisy recovery, conditions for success (RIP, NSP), ℓ1‑based relaxations, support recovery guarantees, and stability under noise.

- **Reproducing Kernel Hilbert Spaces (RKHS)**  
  Positive semidefinite kernels and feature maps, Mercer’s theorem, and Kernel Ridge Regression with generalization guarantees.

- **Minimax Lower Bounds**  
  Information-theoretic techniques including Fano’s and Le Cam’s methods.

---

## How to use

- Start with **Assignment Papers/** to build intuition and see canonical proofs.  
- Work through **Assignments/** to practice theory (expect non‑asymptotic bounds, complexity tools, and proof-oriented questions).  
- Refer to **Presentations/** for lecture summaries and topic overviews.

---

## Credit & License

© Sharif University of Technology — For personal, non-commercial educational use. Please cite the course and instructor if you reuse any materials.

**Contacts:**  
- Instructor: Amir Najafi  
- Head TA: Alireza Mirrokni

